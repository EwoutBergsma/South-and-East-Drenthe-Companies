{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions:\n",
    "- Check the instructions in the README.md in the root of this repository\n",
    "- Run all code in sequence\n",
    "- :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import requests\n",
    "import openpyxl  # TODO: is this used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_mode = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading All the HTML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory to store the html files\n",
    "os.makedirs('html', exist_ok=True)\n",
    "# Use glob to find all the html files in the html directory\n",
    "html_files = glob.glob('google_html/*.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain all company names, their website URLs, and GPS coordinates, from the html files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for html_file in html_files[:3]:\n",
    "    # Load html content from a file \"IT companies south east Drenthe.html\"\n",
    "    with open(html_file, encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Find all parent elements that contain both the title and the URL\n",
    "        parent_elements = soup.find_all(\"div\", class_=\"qBF1Pd fontHeadlineSmall\")\n",
    "\n",
    "        company_names = []  # These are named titles in Google Maps jargon\n",
    "        company_urls = []\n",
    "        seen_urls = {}  # There was a bug in the code that caused duplicate URLs to be added to the list, so we need to keep track of the URLs we have already seen and filter those out. Not the pretiest solution, but it works.\n",
    "        for parent in parent_elements:\n",
    "            title = re.sub(' +', ' ', parent.get_text().replace('\\n', '').strip())\n",
    "            link_tag = parent.find_next(\"a\", class_=\"lcr4fd S9kvJb\")\n",
    "            url = link_tag['href'] if link_tag and 'href' in link_tag.attrs else None\n",
    "            if url in seen_urls:\n",
    "                company_urls[seen_urls[url]] = \"\"\n",
    "            seen_urls[url] = len(company_urls)\n",
    "            company_names.append(title)\n",
    "            company_urls.append(url)\n",
    "\n",
    "        if debug_mode: print(\"Company names:\", company_names)\n",
    "        if debug_mode: print(\"Company URLs:\", company_urls)\n",
    "\n",
    "        # Find the href in the a tag with class \"hfpxzc\"\n",
    "        google_urls = [link['href'] for link in soup.find_all(\"a\", class_=\"hfpxzc\")]\n",
    "        if debug_mode: print(\"Google URLs:\", google_urls)\n",
    "\n",
    "        # Use regular expressions to find latitude and longitude\n",
    "        latitude = [re.search(r'!3d([-.\\d]+)', url) for url in google_urls]\n",
    "        longitude = [re.search(r'!4d([-.\\d]+)', url) for url in google_urls]\n",
    "\n",
    "        # Extract and convert to float\n",
    "        latitude = [float(lat.group(1)) if lat else None for lat in latitude]\n",
    "        longitude = [float(lon.group(1)) if lon else None for lon in longitude]\n",
    "\n",
    "        # Print the GPS coordinates, for debugging purposes\n",
    "        gps_coordinates = [(lat, long) for lat, long in zip(latitude, longitude)]\n",
    "        if debug_mode: print(\"GPS Coordinates:\", gps_coordinates)\n",
    "\n",
    "        # Put the data into a pandas DataFrame\n",
    "        df = pd.DataFrame({'Company name': company_names, 'Company URL': company_urls, 'Google URL': google_urls, 'Latitude': latitude, 'Longitude': longitude})\n",
    "        if debug_mode: print(df)\n",
    "\n",
    "        # Create directory to store a csv file per html file\n",
    "        os.makedirs('csv', exist_ok=True)\n",
    "\n",
    "        # Save the output to a csv file in the csv directory, using the name of the html file\n",
    "        csv_file = os.path.basename(html_file).replace('.html', '.csv')\n",
    "        df.to_csv(os.path.join('csv', csv_file), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visit all Company URLs, save their frontpage as html, and see if they have a description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'html\\\\Arcady | IT Consultancy Zwolle.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Save the html content in the html directory, make folder if it doesn't exist\u001b[39;00m\n\u001b[0;32m     20\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhtml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompany_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.html\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     22\u001b[0m     file\u001b[38;5;241m.\u001b[39mwrite(html_content)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Find the meta tag with the name 'description'\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'html\\\\Arcady | IT Consultancy Zwolle.html'"
     ]
    }
   ],
   "source": [
    "# Load all csv files in the csv directory\n",
    "csv_files = glob.glob('csv/*.csv')\n",
    "\n",
    "for csv in csv_files:\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv)\n",
    "    descriptions = []\n",
    "    \n",
    "    # Iterate over each URL in the 'Company URL' column\n",
    "    for url, company_name in zip(df['Company URL'], df['Company name']):\n",
    "        if url:\n",
    "            try:\n",
    "                # Send a GET request to the URL\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    html_content = response.text\n",
    "                    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "                    # Save the html content in the html directory, make folder if it doesn't exist\n",
    "                    os.makedirs('html', exist_ok=True)\n",
    "                    path = os.path.join('html', company_name + '.html').replace('', '|')\n",
    "                    with open(os.path.join('html', company_name + '.html'), 'w', encoding='utf-8') as file:\n",
    "                        file.write(html_content)\n",
    "                    # Find the meta tag with the name 'description'\n",
    "                    description = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "                    # Append the content of the description meta tag to the descriptions list\n",
    "                    descriptions.append(description['content'] if description else None)\n",
    "                else:\n",
    "                    descriptions.append(None)\n",
    "            except requests.RequestException as e:\n",
    "                # Print the error message if the request fails\n",
    "                if debug_mode: print(f\"Request failed for URL: {url}, error: {e}\")\n",
    "                descriptions.append(None)\n",
    "        else:\n",
    "            descriptions.append(None)\n",
    "\n",
    "    # Add the descriptions list as a new column in the DataFrame\n",
    "    df['Description'] = descriptions\n",
    "\n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df.to_csv(csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the CSVs to one Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all csv files in the csv directory\n",
    "csv_files = glob.glob('csv/*.csv')\n",
    "\n",
    "# Create an Excel file, that will contain all csv files, each in a separate sheet\n",
    "with pd.ExcelWriter('Companies.xlsx') as writer:\n",
    "    for csv in csv_files:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(csv)\n",
    "        # Get the name of the CSV file (without the directory and extension)\n",
    "        sheet_name = os.path.splitext(os.path.basename(csv))[0]\n",
    "        # Write the DataFrame to the Excel file\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
