{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions:\n",
    "- Check the instructions in the README.md in the root of this repository\n",
    "- Run all code in sequence\n",
    "- :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import requests\n",
    "import openpyxl  # TODO: is this used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_mode = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading All the HTML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory to store the html files\n",
    "os.makedirs('html', exist_ok=True)\n",
    "# Use glob to find all the html files in the html directory\n",
    "html_files = glob.glob('google_html/*.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain all company names, their website URLs, and GPS coordinates, from the html files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for html_file in html_files:\n",
    "    # Load html content from a file \"IT companies south east Drenthe.html\"\n",
    "    with open(html_file, encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Find all parent elements that contain both the title and the URL\n",
    "        parent_elements = soup.find_all(\"div\", class_=\"qBF1Pd fontHeadlineSmall\")\n",
    "\n",
    "        company_names = []  # These are named titles in Google Maps jargon\n",
    "        company_urls = []\n",
    "        seen_urls = {}  # There was a bug in the code that caused duplicate URLs to be added to the list, so we need to keep track of the URLs we have already seen and filter those out. Not the pretiest solution, but it works.\n",
    "        for parent in parent_elements:\n",
    "            title = re.sub(' +', ' ', parent.get_text().replace('\\n', '').strip())\n",
    "            link_tag = parent.find_next(\"a\", class_=\"lcr4fd S9kvJb\")\n",
    "            url = link_tag['href'] if link_tag and 'href' in link_tag.attrs else None\n",
    "            if url in seen_urls:\n",
    "                company_urls[seen_urls[url]] = \"\"\n",
    "            seen_urls[url] = len(company_urls)\n",
    "            company_names.append(title)\n",
    "            company_urls.append(url)\n",
    "\n",
    "        if debug_mode: print(\"Company names:\", company_names)\n",
    "        if debug_mode: print(\"Company URLs:\", company_urls)\n",
    "\n",
    "        # Find the href in the a tag with class \"hfpxzc\"\n",
    "        google_urls = [link['href'] for link in soup.find_all(\"a\", class_=\"hfpxzc\")]\n",
    "        if debug_mode: print(\"Google URLs:\", google_urls)\n",
    "\n",
    "        # Use regular expressions to find latitude and longitude\n",
    "        latitude = [re.search(r'!3d([-.\\d]+)', url) for url in google_urls]\n",
    "        longitude = [re.search(r'!4d([-.\\d]+)', url) for url in google_urls]\n",
    "\n",
    "        # Extract and convert to float\n",
    "        latitude = [float(lat.group(1)) if lat else None for lat in latitude]\n",
    "        longitude = [float(lon.group(1)) if lon else None for lon in longitude]\n",
    "\n",
    "        # Print the GPS coordinates, for debugging purposes\n",
    "        gps_coordinates = [(lat, long) for lat, long in zip(latitude, longitude)]\n",
    "        if debug_mode: print(\"GPS Coordinates:\", gps_coordinates)\n",
    "\n",
    "        # Put the data into a pandas DataFrame\n",
    "        df = pd.DataFrame({'Company name': company_names, 'Company URL': company_urls, 'Google URL': google_urls, 'Latitude': latitude, 'Longitude': longitude})\n",
    "        if debug_mode: print(df)\n",
    "\n",
    "        # Create directory to store a csv file per html file\n",
    "        os.makedirs('csv', exist_ok=True)\n",
    "\n",
    "        # Save the output to a csv file in the csv directory, using the name of the html file\n",
    "        csv_file = os.path.basename(html_file).replace('.html', '.csv')\n",
    "        df.to_csv(os.path.join('csv', csv_file), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visit all Company URLs, save their frontpage as html, and see if they have a description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all csv files in the csv directory\n",
    "csv_files = glob.glob('csv/*.csv')\n",
    "\n",
    "for csv in csv_files:\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv)\n",
    "    descriptions = []\n",
    "    \n",
    "    # Iterate over each URL in the 'Company URL' column\n",
    "    for url, company_name in zip(df['Company URL'], df['Company name']):\n",
    "        if url:\n",
    "            try:\n",
    "                # Send a GET request to the URL\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    html_content = response.text\n",
    "                    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "                    # Save the html content in the html directory, make folder if it doesn't exist\n",
    "                    path = os.path.join('html', os.path.basename(csv).replace('.csv', ''))\n",
    "                    path = os.path.join(path, company_name + '.html').replace('|', '')\n",
    "                    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "                    with open(path, 'w', encoding='utf-8') as file:\n",
    "                        file.write(html_content)\n",
    "                    # Find the meta tag with the name 'description'\n",
    "                    description = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "                    # Append the content of the description meta tag to the descriptions list\n",
    "                    descriptions.append(description['content'] if description else None)\n",
    "                else:\n",
    "                    descriptions.append(None)\n",
    "            except requests.RequestException as e:\n",
    "                # Print the error message if the request fails\n",
    "                if debug_mode: print(f\"Request failed for URL: {url}, error: {e}\")\n",
    "                descriptions.append(None)\n",
    "        else:\n",
    "            descriptions.append(None)\n",
    "\n",
    "    # Add the descriptions list as a new column in the DataFrame\n",
    "    df['Description'] = descriptions\n",
    "\n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df.to_csv(csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the CSVs to one Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all csv files in the csv directory\n",
    "csv_files = glob.glob('csv/*.csv')\n",
    "\n",
    "# Create an Excel file, that will contain all csv files, each in a separate sheet\n",
    "with pd.ExcelWriter('Companies.xlsx') as writer:\n",
    "    for csv in csv_files:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(csv)\n",
    "        # Get the name of the CSV file (without the directory and extension)\n",
    "        sheet_name = os.path.splitext(os.path.basename(csv))[0]\n",
    "        # Write the DataFrame to the Excel file\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract text from the downloaded html frontpages, put them in csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmls = glob.glob(\"html/*/*.html\")\n",
    "\n",
    "def extract_text_from_html(html_content):\n",
    "    \"\"\"\n",
    "    Extracts all human-readable text from an HTML document.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    # Remove script and style elements\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"meta\", \"link\", \"head\"]):\n",
    "        tag.extract()\n",
    "\n",
    "    # Get text and clean up unnecessary whitespace\n",
    "    text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    return text\n",
    "\n",
    "# List to store extracted texts\n",
    "data = []\n",
    "\n",
    "# Loop through HTML files and extract text\n",
    "for html in htmls:\n",
    "    with open(html, \"r\", encoding=\"utf-8\") as file:\n",
    "        html_content = file.read()\n",
    "    extracted_text = extract_text_from_html(html_content).replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    \n",
    "    # Append to data list\n",
    "    data.append({\"Filename\": os.path.basename(html), \"ExtractedText\": extracted_text})\n",
    "\n",
    "# Convert list to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv(\"extracted_texts.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>ExtractedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IoT Nederland.html</td>\n",
       "      <td>IoT Nederland Home Hoe werkt het Wat doen wij ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Let Things Talk BV.html</td>\n",
       "      <td>0512-208000 Office Ampérelaan 3, 9207 AM, Drac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Things Network.html</td>\n",
       "      <td>We are a global collaborative Internet of Thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thingsdata.html</td>\n",
       "      <td>Skip to content +31 (0)85-0443500 info@thingsd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BACE IoT.html</td>\n",
       "      <td>Smart Energy Body Monitoring Compliance Report...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Filename                                      ExtractedText\n",
       "0       IoT Nederland.html  IoT Nederland Home Hoe werkt het Wat doen wij ...\n",
       "1  Let Things Talk BV.html  0512-208000 Office Ampérelaan 3, 9207 AM, Drac...\n",
       "2  The Things Network.html  We are a global collaborative Internet of Thin...\n",
       "3          Thingsdata.html  Skip to content +31 (0)85-0443500 info@thingsd...\n",
       "4            BACE IoT.html  Smart Energy Body Monitoring Compliance Report..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open extracted_texts.csv and read the content\n",
    "df = pd.read_csv(\"extracted_texts.csv\")\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
