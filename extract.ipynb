{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions:\n",
    "- Check the instructions in the README.md in the root of this repository\n",
    "- Run all code in sequence\n",
    "- :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import requests\n",
    "import openpyxl  # TODO: is this used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_mode = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading All the HTML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory to store the html files\n",
    "os.makedirs('html', exist_ok=True)\n",
    "# Use glob to find all the html files in the html directory\n",
    "html_files = glob.glob('html/*.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain all company names, their website URLs, and GPS coordinates, from the html files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for html_file in html_files:\n",
    "    # Load html content from a file \"IT companies south east Drenthe.html\"\n",
    "    with open(html_file, encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Find all parent elements that contain both the title and the URL\n",
    "        parent_elements = soup.find_all(\"div\", class_=\"qBF1Pd fontHeadlineSmall\")\n",
    "\n",
    "        company_names = []  # These are named titles in Google Maps jargon\n",
    "        company_urls = []\n",
    "        seen_urls = {}  # There was a bug in the code that caused duplicate URLs to be added to the list, so we need to keep track of the URLs we have already seen and filter those out. Not the pretiest solution, but it works.\n",
    "        for parent in parent_elements:\n",
    "            title = re.sub(' +', ' ', parent.get_text().replace('\\n', '').strip())\n",
    "            link_tag = parent.find_next(\"a\", class_=\"lcr4fd S9kvJb\")\n",
    "            url = link_tag['href'] if link_tag and 'href' in link_tag.attrs else None\n",
    "            if url in seen_urls:\n",
    "                company_urls[seen_urls[url]] = \"\"\n",
    "            seen_urls[url] = len(company_urls)\n",
    "            company_names.append(title)\n",
    "            company_urls.append(url)\n",
    "\n",
    "        if debug_mode: print(\"Company names:\", company_names)\n",
    "        if debug_mode: print(\"Company URLs:\", company_urls)\n",
    "\n",
    "        # Find the href in the a tag with class \"hfpxzc\"\n",
    "        google_urls = [link['href'] for link in soup.find_all(\"a\", class_=\"hfpxzc\")]\n",
    "        if debug_mode: print(\"Google URLs:\", google_urls)\n",
    "\n",
    "        # Use regular expressions to find latitude and longitude\n",
    "        latitude = [re.search(r'!3d([-.\\d]+)', url) for url in google_urls]\n",
    "        longitude = [re.search(r'!4d([-.\\d]+)', url) for url in google_urls]\n",
    "\n",
    "        # Extract and convert to float\n",
    "        latitude = [float(lat.group(1)) if lat else None for lat in latitude]\n",
    "        longitude = [float(lon.group(1)) if lon else None for lon in longitude]\n",
    "\n",
    "        # Print the GPS coordinates, for debugging purposes\n",
    "        gps_coordinates = [(lat, long) for lat, long in zip(latitude, longitude)]\n",
    "        if debug_mode: print(\"GPS Coordinates:\", gps_coordinates)\n",
    "\n",
    "        # Put the data into a pandas DataFrame\n",
    "        df = pd.DataFrame({'Company name': company_names, 'Company URL': company_urls, 'Google URL': google_urls, 'Latitude': latitude, 'Longitude': longitude})\n",
    "        if debug_mode: print(df)\n",
    "\n",
    "        # Create directory to store a csv file per html file\n",
    "        os.makedirs('csv', exist_ok=True)\n",
    "\n",
    "        # Save the output to a csv file in the csv directory, using the name of the html file\n",
    "        csv_file = os.path.basename(html_file).replace('.html', '.csv')\n",
    "        df.to_csv(os.path.join('csv', csv_file), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visit all Company URLs and see if they have a description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all csv files in the csv directory\n",
    "csv_files = glob.glob('csv/*.csv')\n",
    "\n",
    "for csv in csv_files:\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv)\n",
    "    descriptions = []\n",
    "    \n",
    "    # Iterate over each URL in the 'Company URL' column\n",
    "    for url in df['Company URL']:\n",
    "        if url:\n",
    "            try:\n",
    "                # Send a GET request to the URL\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    html_content = response.text\n",
    "                    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "                    # Find the meta tag with the name 'description'\n",
    "                    description = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "                    # Append the content of the description meta tag to the descriptions list\n",
    "                    descriptions.append(description['content'] if description else None)\n",
    "                else:\n",
    "                    descriptions.append(None)\n",
    "            except requests.RequestException as e:\n",
    "                # Print the error message if the request fails\n",
    "                if debug_mode: print(f\"Request failed for URL: {url}, error: {e}\")\n",
    "                descriptions.append(None)\n",
    "        else:\n",
    "            descriptions.append(None)\n",
    "\n",
    "    # Add the descriptions list as a new column in the DataFrame\n",
    "    df['Description'] = descriptions\n",
    "\n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df.to_csv(csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the CSVs to one Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all csv files in the csv directory\n",
    "csv_files = glob.glob('csv/*.csv')\n",
    "\n",
    "# Create an Excel file, that will contain all csv files, each in a separate sheet\n",
    "with pd.ExcelWriter('companies.xlsx') as writer:\n",
    "    for csv in csv_files:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(csv)\n",
    "        # Get the name of the CSV file (without the directory and extension)\n",
    "        sheet_name = os.path.splitext(os.path.basename(csv))[0]\n",
    "        # Write the DataFrame to the Excel file\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
